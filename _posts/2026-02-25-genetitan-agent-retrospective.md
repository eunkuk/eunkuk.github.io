---
title: "Python 스크립트 하나로 돌리던 유전자 분석을 데스크톱 앱으로 만들기까지 — GeneTitan Agent 회고"
date: 2026-02-25 22:00:00 +0900
categories: [Dev, Architecture]
tags: [tauri, rust, typescript, bioinformatics, desktop-app, pipeline-automation]
description: "array_db.py 하나로 돌리던 GeneTitan 마이크로어레이 분석 파이프라인을, Tauri + Rust + React로 데스크톱 앱을 만들게 된 1인 개발자의 기록."
---

## 상황

[마이그레이션 회고](/posts/php-codeigniter-spring-boot-migration-why/)에서 PHP → Spring Boot 전환을 다뤘다. 어드민을 개발하면서 유전자 데이터 업로드 과정이 눈에 들어왔다. 연구원이 분석 파이프라인을 수동으로 돌리고, 결과 파일을 어드민에 직접 업로드하는 구조 — 업로드를 받는 어드민은 새로 만들었는데, 그 앞단의 분석 과정은 여전히 수동이었다. 이 과정 자체를 자동화할 수 있지 않을까?

유전자검사 서비스를 운영하고 있다. 검체가 랩에 도착하면 마이크로어레이 칩(Axiom_APMRA_24)에 올리고, GeneTitan 장비가 CEL 파일을 뽑아낸다. 여기까지는 장비가 알아서 한다.

문제는 그 다음이다. CEL 파일이 나오면 12단계의 분석 파이프라인을 돌려야 한다. QC 분석, DQC 필터링, genotyping, Call Rate 필터링, VCF 생성, Probeset 필터링, 클러스터 플롯까지. 이걸 처리하는 도구가 APT(Affymetrix Power Tools)와 R/SNPolisher인데, 각 단계마다 명령어가 다르고, 입력 파일과 출력 파일이 다르고, 임계값 설정이 다르다.

```
CEL 파일 → QC 분석 → DQC 필터링 (≥0.82) → Step1 Genotyping →
Call Rate 필터링 (≥97%) → Plate QC → Step2 Genotyping →
ps-metrics → ps-classification → VCF 생성 → VCF 필터링 →
Cluster Plot Visualization
```

이 파이프라인을 돌리는 방법은 `array_db.py`라는 Python 스크립트 하나였다. 이전 담당자가 만든 것이었고, 서버에서 CLI로 실행하는 방식이었다.

## array_db.py의 한계

스크립트 자체는 돌아갔다. 문제는 운영이었다.

### 블랙박스 실행

스크립트를 실행하면 터미널에 로그가 흘러간다. 지금 12단계 중 어디쯤인지, 몇 퍼센트 진행됐는지, 어떤 샘플이 필터링됐는지 — 실시간으로 알 수 있는 방법이 없었다. 끝날 때까지 터미널을 보고 있거나, 끝나고 나서 로그 파일을 뒤져봐야 했다.

```bash
# 이게 전부였다
python array_db.py --cel-path /data/cel_files --output /data/results
# ... 10분~30분 대기 ...
# 끝나면 로그 확인
```

분석이 실패했을 때가 문제다. 어느 단계에서 실패했는지, 어떤 파라미터가 잘못됐는지를 로그 파일에서 찾아야 했다. DQC 임계값을 바꿔서 다시 돌리고 싶으면 스크립트 내부를 직접 수정하거나 인자를 찾아서 바꿔야 했다.

### Probeset 관리의 고통

VCF 필터링에는 Probeset ID 목록이 필요하다. 어떤 SNP를 필터링할 것인지 정의하는 파일인데, 검사 항목이 추가되거나 변경될 때마다 이 목록을 수정해야 했다.

텍스트 파일 하나를 열고, ID를 찾아서 추가하거나 삭제하고, 중복이 없는지 확인하고, 저장한다. 실수로 ID를 잘못 넣으면 검사 결과가 달라진다. 백업? 수동으로 복사해놓거나 git으로 관리하는 수밖에 없었다.

### Windows에서는 못 돌린다

`array_db.py`는 Linux 서버에서만 돌아갔다. APT 바이너리가 Linux 전용이었기 때문이다. 랩 담당자가 분석 결과를 확인하려면 서버에 SSH로 접속해야 했고, CLI에 익숙하지 않은 사람은 결과 파일을 열어보는 것조차 어려웠다.

| 문제 | 영향 |
|------|------|
| 실시간 진행률 없음 | 분석이 끝날 때까지 대기, 실패 시 원인 추적 어려움 |
| 설정 변경 불편 | 스크립트 내부 수정 또는 CLI 인자 수동 입력 |
| Probeset 관리 수동 | 텍스트 파일 직접 편집, 실수 가능성 |
| Windows 미지원 | 랩 담당자는 CLI 접근 불가 |
| 백업/복원 없음 | 설정이나 Probeset 변경 후 되돌리기 어려움 |

> 파이프라인 자체는 잘 돌아간다. 문제는 파이프라인을 **운영**하는 것이다. 설정을 바꾸고, 결과를 확인하고, 문제가 생겼을 때 빠르게 대응하는 것. `array_db.py`는 "돌리는 도구"였지 "운영하는 도구"가 아니었다.

## 왜 데스크톱 앱인가

`array_db.py`를 개선하는 방법은 여러 가지였다. 스크립트를 리팩토링하거나, 웹 UI를 붙이거나, 서버에서 돌리는 구조를 유지하면서 모니터링만 추가하거나. 데스크톱 앱을 만들기로 한 건 현장의 조건 때문이었다.

### 워크스테이션이 윈도우다

GeneTitan 장비에 연결된 워크스테이션의 OS가 Windows다. 장비가 CEL 파일을 뽑아내면 이 워크스테이션의 로컬 디스크에 저장된다.

기존 `array_db.py`는 Linux 서버에서만 돌아갔다. 그래서 분석을 하려면 CEL 파일을 워크스테이션에서 Linux 서버로 옮겨야 했다. 파일을 옮기고, 서버에 SSH로 접속해서, 스크립트를 실행하고, 결과 파일을 다시 가져오는 과정이 매번 반복됐다.

```
[GeneTitan 장비] → CEL 파일 → [Windows 워크스테이션]
                                      │
                              SCP/SFTP로 복사 (수동)
                                      ↓
                              [Linux 서버] → array_db.py 실행
                                      │
                              결과 파일 다시 복사 (수동)
                                      ↓
                              [Windows 워크스테이션] → 결과 확인
```

Windows에서 바로 돌릴 수 있으면 이 왕복이 사라진다. 파이프라인 도구가 워크스테이션에서 직접 실행되어야 한다는 게 첫 번째 조건이었다.

### 서버에 올리는 건 비용이다

CEL 파일을 서버로 옮겨서 분석하는 건, 생각보다 비용이 크다.

| 항목 | 서버에서 분석 | 워크스테이션에서 분석 |
|------|-------------|-------------------|
| CEL 파일 전송 | 매번 SCP/SFTP로 복사 | 불필요 (로컬 디스크) |
| 네트워크 의존 | 네트워크 끊기면 분석 불가 | 오프라인에서도 가능 |
| 서버 부하 | 분석 작업이 서버 자원 사용 | 워크스테이션 자원 사용 |
| 결과 전달 | 결과 파일 다시 복사 | 로컬에서 바로 확인 |
| 서버에 올리는 데이터 | CEL 파일 전체 (~수백 MB) | 최종 결과값만 (~수 KB) |

CEL 파일 전체를 서버에 올리는 대신, 워크스테이션에서 분석을 끝내고 최종 결과값만 서버에 올리는 게 효율적이었다. 네트워크 비용도 줄고, 서버 부하도 줄고, 파이프라인이 네트워크 상태에 의존하지 않게 된다.

### 장비 앞에서 확인할 수 있어야 한다

연구원은 GeneTitan 장비 앞에서 작업한다. 칩을 장비에 넣고, CEL 파일이 나오면, 분석을 돌리고, 결과를 확인한다. 이 흐름이 한 자리에서 이루어져야 한다.

기존에는 서버에 SSH로 접속해서 CLI로 스크립트를 실행하고, 로그 파일을 열어서 결과를 확인했다. 터미널에 익숙한 사람이면 괜찮지만, 모든 연구원이 그렇지는 않다. 분석이 12단계 중 어디까지 진행됐는지, 어떤 샘플이 QC에서 탈락했는지, 클러스터 플롯은 어떻게 나왔는지 — 이런 정보를 시각적으로 바로 확인할 수 있어야 했다.

> 도구의 사용자가 개발자가 아니라면, CLI보다 GUI가 맞다. "파이프라인 상태를 한눈에 보여주는 화면"은 터미널 로그 수십 줄보다 낫다.

### 실패하면 바로 다시 돌릴 수 있어야 한다

파이프라인은 실패한다. DQC 임계값이 너무 높아서 샘플이 전부 탈락하거나, CEL 파일이 손상됐거나, R 스크립트 경로가 바뀌었거나. 이유는 다양하다.

기존 방식에서 재실행은 이랬다. 서버에 SSH 접속 → 로그 파일 열어서 실패 원인 확인 → 스크립트 인자나 내부 코드 수정 → 다시 실행. 연구원이 이걸 혼자 하기는 어렵다.

GUI가 있으면 달라진다. 어느 단계에서 실패했는지 화면에 바로 보이고, 임계값을 설정 탭에서 바꾸고, 버튼 하나로 다시 돌릴 수 있다. 에러 대응의 진입장벽이 낮아진다.

## 왜 Tauri인가

데스크톱 앱 프레임워크를 골라야 했다. 선택지는 크게 Electron과 Tauri였다.

### Electron은 써봤다

Electron으로 앱을 만들어본 경험이 있었다. 잘 동작했고, 생태계도 성숙하다. 그런데 이 프로젝트에는 맞지 않았다.

GeneTitan Agent는 APT 바이너리(~300MB), R 패키지, Axiom 라이브러리 파일을 같이 배포해야 한다. Electron은 Chromium 전체를 번들링하기 때문에 아무것도 안 해도 100MB가 넘는다. 여기에 Chromium까지 얹으면 설치 파일이 500MB를 넘길 판이었다.

| 비교 항목 | Electron | Tauri |
|----------|----------|-------|
| 앱 크기 | ~100MB+ | ~4-13MB |
| 메모리 사용 | ~500MB | ~50-100MB |
| 백엔드 언어 | Node.js | **Rust** |
| 프로세스 실행 | child_process | **std::process::Command** |
| 파일 시스템 | fs 모듈 | **Rust std::fs** |

랩 워크스테이션은 사양이 넉넉하지 않은 경우가 많다. 분석 장비 옆에 붙어있는 PC에서 Electron 앱이 메모리를 500MB씩 잡아먹으면 분석 자체에 영향을 줄 수 있었다. Tauri는 앱 자체가 4-13MB이고, 메모리 사용량도 Electron의 1/5 수준이다. 랩 환경에는 Tauri가 맞았다.

> 프레임워크 선택은 기술적 우열이 아니라 **배포 환경**에 맞는지가 먼저다. Electron이 나쁜 선택은 아니었지만, APT 바이너리와 함께 배포해야 하는 상황에서 Chromium까지 얹는 건 부담이었다.

### Rust를 해보고 싶었다

솔직한 동기가 하나 더 있었다. Rust를 실무 프로젝트에 써보고 싶었다.

Tauri의 백엔드는 Rust다. GeneTitan Agent의 백엔드가 하는 일 — APT 바이너리를 `Command`로 실행하고, TSV 출력 파일을 파싱하고, DQC/Call Rate 필터링 로직을 돌리고, R 스크립트를 호출하는 것 — 은 파일 I/O와 프로세스 관리가 대부분이다. Rust가 잘하는 영역이기도 했다.

```rust
pub fn filter_by_dqc(
    qc_file: &Path,
    threshold: f64,
    output_path: &Path,
) -> Result<FilterResult> {
    let reader = BufReader::new(File::open(qc_file)?);
    let mut passed = Vec::new();
    let mut failed = Vec::new();

    for line in reader.lines() {
        let line = line?;
        // TSV 파싱 → DQC 값 추출 → 임계값 비교
        let dqc: f64 = fields[dqc_col].parse()?;
        if dqc >= threshold {
            passed.push(cel_path);
        } else {
            failed.push((cel_path, dqc));
        }
    }
    // ...
}
```

유전자검사 데이터는 틀리면 안 된다. 필터링에서 한 샘플이 빠지거나 잘못 포함되면 검사 결과가 달라진다. Rust의 타입 시스템과 에러 핸들링(`Result`, `Option`)이 이런 실수를 컴파일 타임에 잡아주는 건, 사후적으로 보면 좋은 선택이었다. 하지만 처음부터 "정확성 때문에 Rust를 골랐다"고 말하면 포장이다. **새 언어를 실무에 써보고 싶었고, 마침 적합한 프로젝트가 있었다.** 그게 솔직한 이유다.

> 기술 선택에 개인적 동기가 섞이는 건 자연스러운 일이다. 중요한 건 그 동기가 프로젝트에 해가 되지 않는지, 기술적으로도 합리적인 선택인지를 같이 따져보는 것이다.

## 현재 구조

최종적으로 정리된 구조다.

```
genetitan-agent/
├── src-tauri/                  # Rust 백엔드 (Tauri 2.1)
│   ├── src/
│   │   ├── main.rs            # 진입점, IPC 핸들러 등록
│   │   ├── types.rs           # AppConfig, 타입 정의
│   │   ├── commands/          # IPC 명령어 모듈 (8개)
│   │   │   ├── pipeline.rs    # 12단계 파이프라인 오케스트레이션
│   │   │   ├── probeset.rs    # Probeset 관리 (CRUD + 백업)
│   │   │   ├── config.rs      # YAML 설정 읽기/쓰기
│   │   │   ├── apt.rs         # APT 바이너리 실행
│   │   │   ├── r_analysis.rs  # R 스크립트 실행
│   │   │   ├── agent.rs       # 에이전트 상태 모니터링
│   │   │   ├── results.rs     # 결과 파일 탐색
│   │   │   └── ui.rs          # 파일/폴더 다이얼로그
│   │   └── utils/             # 유틸리티 (경로, 로깅, 필터링)
│   └── Cargo.toml
├── src/                        # React 프론트엔드
│   ├── components/tabs/       # 5개 탭 컴포넌트
│   ├── hooks/                 # useConfig, useTauriCommand
│   └── App.tsx
├── r-scripts/                  # R 분석 스크립트 (3개)
├── config.yml                  # 런타임 설정
└── docs/                       # 기술 문서
```

Tauri의 IPC(Inter-Process Communication)가 프론트엔드와 백엔드를 연결한다. React 컴포넌트에서 `invoke('execute_auto_pipeline', { ... })`을 호출하면 Rust의 `execute_auto_pipeline` 함수가 실행되고, `emit('pipeline-progress', ...)`로 실시간 진행률을 프론트엔드에 쏴준다.

```
[React Frontend]  ──invoke──>  [Rust Backend]  ──Command──>  [APT/R/Perl]
       ↑                            │
       └──────── emit (이벤트) ──────┘
```

## 대신 포기한 것들

### 크로스 플랫폼 빌드의 고통

Tauri는 크로스 컴파일을 공식 지원하지 않는다. Windows 빌드는 Windows에서, Linux 빌드는 Linux에서 해야 한다. CI/CD를 제대로 구성하려면 GitHub Actions에 양쪽 OS runner를 둬야 한다.

지금은 로컬에서 각 OS별로 빌드하고 있다. 1인 개발이라 감당할 수 있지만, 팀이 커지면 빌드 파이프라인을 정리해야 할 부분이다.

### 번들 크기

앱 자체는 4-13MB로 가볍다. 하지만 APT 바이너리, R 패키지, Axiom 라이브러리 파일까지 포함한 Full 패키지는 400MB에 달한다. Lite 버전(APT 별도 설치)으로 50MB까지 줄일 수 있지만, 설치 과정이 복잡해진다.

```bash
# Full Edition: ~400MB (APT 바이너리 포함)
genetitan-agent-full-1.0.0.deb

# Lite Edition: ~50MB (APT 별도 설치 필요)
genetitan-agent-lite-1.0.0.deb
```

### 테스트 부재

솔직히 말하면 자동화된 테스트가 없다. 파이프라인의 각 단계가 외부 바이너리(APT)를 실행하는 구조라서, 단위 테스트를 작성하려면 APT 출력을 모킹해야 한다. 필터링 유틸리티 함수는 테스트할 수 있지만, 파이프라인 전체 흐름은 실제 CEL 파일 없이는 검증할 수 없다.

지금은 수동 테스트로 버티고 있다. 규모가 커지면 통합 테스트 환경을 구축해야 한다.

## 돌아보며

`array_db.py`에서 GeneTitan Agent로의 전환은 **"돌리는 도구"에서 "운영하는 도구"로의 전환**이었다.

스크립트 하나를 데스크톱 앱으로 만드는 건 과한 것처럼 보일 수 있다. 하지만 유전자검사 파이프라인은 한 번 돌리고 끝나는 게 아니다. 검체가 들어올 때마다 돌려야 하고, 임계값을 조정해야 하고, Probeset을 관리해야 하고, 결과를 확인해야 한다. 이 반복 작업의 마찰을 줄이는 것이 앱의 존재 이유다.

돌아보면, 데스크톱 앱이라는 결정과 Tauri라는 선택은 별개의 판단이었다.

데스크톱 앱은 **현장의 조건**이 결정했다. 워크스테이션이 윈도우고, CEL 파일이 로컬에 있고, 연구원이 장비 앞에서 확인해야 하고, 실패하면 바로 다시 돌릴 수 있어야 했다. 이 네 가지가 웹도 CLI도 아닌 데스크톱 앱을 가리켰다.

Tauri는 **환경적 제약과 개인적 동기**가 같이 작용했다. APT 바이너리와 함께 배포해야 하는 상황에서 Electron의 번들 크기는 부담이었고, 랩 워크스테이션의 사양을 생각하면 가벼운 쪽이 나았다. 거기에 Rust를 실무에 써보고 싶었다는 동기가 겹쳤다.

> 1. **현장이 구조를 결정한다** — 윈도우 워크스테이션, 로컬 CEL 파일, 비개발자 사용자. 이 조건들이 데스크톱 앱이라는 형태를 결정했다
> 2. **배포 환경이 프레임워크를 결정한다** — APT 바이너리 300MB와 함께 배포해야 하는 상황에서, 프레임워크가 100MB를 더 얹는 건 부담이다
> 3. **기술 선택에 솔직해야 한다** — Rust를 써보고 싶었다. 마침 파일 I/O와 프로세스 관리가 많은 프로젝트라 적합하기도 했다. 둘 다 이유다

Python 스크립트 하나로 충분한 순간이 있고, 데스크톱 앱이 필요한 순간이 있다. GeneTitan Agent는 후자였다. 검체가 들어올 때마다 터미널을 열고 명령어를 타이핑하는 대신, 버튼 하나로 파이프라인을 돌리고 실시간으로 진행률을 확인할 수 있게 된 것. 그게 이 앱을 만든 이유다.

---

**이전 글:** [마이그레이션 회고 (5) — 교훈과 수치](/posts/migration-lessons-learned-retrospective/) — 이 글의 출발점이 된 어드민 개발 회고
